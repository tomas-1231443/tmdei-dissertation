\chapter{Raw Outputs: Version 1 vs. Version 11}
\label{AppendixA}

This appendix presents the raw outputs of the models for Version 1 and Version 11, which were used for the evaluation and analysis in Chapter 4, Section 4.1. These outputs include the confusion matrices and classification metrics for both priority and taxonomy classifications.

\section*{Version 1 Performance Metrics}

\subsection*{Priority Classification}

\begin{table}[h!]
    \centering
    \caption{Version 1 Priority Classification Metrics}
    \label{tab:v1_priority_metrics}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
    \hline
    P1 & 0.81 & 0.75 & 0.78 & 4162 \\
    \hline
    P2 & 0.81 & 0.77 & 0.79 & 9374\\
    \hline
    P3 & 0.83 & 0.89 & 0.86 & 12462 \\
    \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Overall Accuracy:} 82\%
    \item \textbf{Macro Average:} Precision 0.82, Recall 0.80, F1-Score 0.81
    \item \textbf{Weighted Average:} Precision 0.82, Recall 0.82, F1-Score 0.82
\end{itemize}

\subsection*{Taxonomy Classification}

\begin{table}[h!]
    \centering
    \caption{Version 1 Taxonomy Classification Metrics}
    \label{tab:v1_taxonomy_metrics}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
    \hline
    0 & 0.82 & 0.19 & 0.31 & 394 \\
    \hline
    1 & 0.94 & 0.86 & 0.90 & 1117 \\
    \hline
    2 & 0.95 & 0.95 & 0.95 & 5937 \\
    \hline
    3 & 0.78 & 0.47 & 0.59 & 1066 \\
    \hline
    4 & 0.72 & 0.84 & 0.78 & 6051 \\
    \hline
    5 & 0.78 & 0.73 & 0.75 & 3467 \\
    \hline
    6 & 0.66 & 0.54 & 0.59 & 1890 \\
    \hline
    7 & 0.86 & 0.92 & 0.89 & 5988 \\
    \hline
    8 & 0.74 & 0.36 & 0.49 & 88 \\
    \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Overall Accuracy:} 82\%
    \item \textbf{Macro Average:} Precision 0.81, Recall 0.65, F1-Score 0.69
    \item \textbf{Weighted Average:} Precision 0.82, Recall 0.82, F1-Score 0.82
\end{itemize}

\newpage

\section*{Version 11 Performance Metrics}

\subsection*{Priority Classification}

\begin{table}[h!]
    \centering
    \caption{Version 11 Priority Classification Metrics}
    \label{tab:v11_priority_metrics}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
    \hline
    P1 & 0.82 & 0.91 & 0.86 & 9511 \\
    \hline
    P2 & 0.89 & 0.85 & 0.87 & 18294 \\
    \hline
    P3 & 0.92 & 0.91 & 0.92 & 24287 \\
    \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Overall Accuracy:} 90\%
    \item \textbf{Macro Average:} Precision 0.89, Recall 0.89, F1-Score 0.88
    \item \textbf{Weighted Average:} Precision 0.89, Recall 0.89, F1-Score 0.89
\end{itemize}

\subsection*{Taxonomy Classification}

\begin{table}[h!]
    \centering
    \caption{Version 11 Taxonomy Classification Metrics}
    \label{tab:v11_taxonomy_metrics}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
    \hline
    0 & 0.95 & 0.95 & 0.95 & 5788 \\
    \hline
    1 & 0.96 & 0.97 & 0.97 & 5788 \\
    \hline
    2 & 0.96 & 0.94 & 0.95 & 5788 \\
    \hline
    3 & 0.90 & 0.94 & 0.92 & 5788 \\
    \hline
    4 & 0.78 & 0.73 & 0.76 & 5787 \\
    \hline
    5 & 0.86 & 0.84 & 0.85 & 5788 \\
    \hline
    6 & 0.87 & 0.89 & 0.88 & 5788 \\
    \hline
    7 & 0.89 & 0.89 & 0.89 & 5788 \\
    \hline
    8 & 0.95 & 0.99 & 0.97 & 5789 \\
    \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Overall Accuracy:} 90\%
    \item \textbf{Macro Average:} Precision 0.90, Recall 0.90, F1-Score 0.90
    \item \textbf{Weighted Average:} Precision 0.90, Recall 0.90, F1-Score 0.90
\end{itemize}

\chapter{Daily Prediction Accuracy During Live Testing}
\label{AppendixB}

This appendix presents the daily statistics on model prediction accuracy during the 15-day live testing period described in Chapter~\ref{chap:Chapter4}. 

The values were derived from a CSV file exported directly from the SOAR platform, containing feedback data for each alert processed by the system. 
The data includes the number of predictions where both the priority and taxonomy labels matched the values later submitted by the analysts (considered as "correct" predictions), as well as the total number of predictions made per day.

\begin{table}[h!]
\centering
\caption{Daily Prediction Statistics During Live Testing}
\label{tab:live_daily_accuracy}
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Day} & \textbf{Correct Predictions} & \textbf{Accuracy} \\
\midrule
0 & 23 / 47 & 48.9\% \\
1 & 27 / 53 & 50.9\% \\
2 & 2 / 67 & 3.0\% \\
3 & 1 / 17 & 5.9\% \\
4 & 0 / 9 & 0.0\% \\
5 & 6 / 47 & 12.8\% \\
6 & 1 / 28 & 3.6\% \\
7 & 1 / 28 & 3.6\% \\
10 & 9 / 17 & 52.9\% \\
11 & 2 / 13 & 15.4\% \\
12 & 19 / 69 & 27.5\% \\
13 & 18 / 62 & 29.0\% \\
14 & 2 / 58 & 3.4\% \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
    \centering
    \caption{Live Priority Classification Metrics}
    \label{tab:live_priority_metrics}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
    \hline
    P1 & 0.30 & 0.13 & 0.18 & 47 \\
    \hline
    P2 & 0.41 & 0.28 & 0.34 & 209 \\
    \hline
    P3 & 0.50 & 0.68 & 0.58 & 259 \\
    \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Overall Accuracy:} 47\%
    \item \textbf{Macro Average:} Precision 0.41, Recall 0.36, F1-Score 0.36
    \item \textbf{Weighted Average:} Precision 0.45, Recall 0.47, F1-Score 0.44
\end{itemize}

\begin{table}[h!]
    \centering
    \caption{Live Taxonomy Classification Metrics}
    \label{tab:live_taxonomy_metrics}
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
    \hline
    Availability & 0.90 & 0.20 & 0.33 & 45 \\
    \hline
    Fraud & 0.82 & 0.44 & 0.57 & 126 \\
    Information Gathering & 0.00 & 0.00 & 0.00 & 13 \\
    \hline
    Information Security & 0.25 & 0.71 & 0.37 & 94 \\
    Intrusion Attempts & 0.26 & 0.29 & 0.27 & 62 \\
    Intrusions & 0.38 & 0.15 & 0.21 & 40 \\
    Malicious Code & 0.95 & 0.44 & 0.60 & 132 \\
    Other & 0.00 & 0.00 & 0.00 & 2 \\
    Vulnerable & 0.00 & 0.00 & 0.00 & 1 \\
    \hline
    \end{tabular}
\end{table}

\begin{itemize}
    \item \textbf{Overall Accuracy:} 41\%
    \item \textbf{Macro Average:} Precision 0.39, Recall 0.25, F1-Score 0.26
    \item \textbf{Weighted Average:} Precision 0.63, Recall 0.41, F1-Score 0.44
\end{itemize}